{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2009a40c",
   "metadata": {},
   "source": [
    "Nil Agell u172941\n",
    "\n",
    "Jordi Badia u173484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29d290",
   "metadata": {},
   "source": [
    "# PART 1 - Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edec1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d1d20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68a1be",
   "metadata": {},
   "source": [
    "Abrimos el documento json y leemos linea a linea. Una vez leidas las lineas, cargamos el json y podremos comprobar que hay un total de 400 tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0020c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 4000\n"
     ]
    }
   ],
   "source": [
    "docs_path = '/Users/jordi/Documents/UNI/4rt Curs/1r Trim/IRWA/P1-IRWA/tw_hurricane_data.json'\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "print('Number of tweets:', len(lines))\n",
    "lines = [json.loads(l) for l in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba002b",
   "metadata": {},
   "source": [
    "Tambien abrimos el documento csv, que nos permitir√° hacer un map de los tweets id con los document ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74340a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>1575918182698979328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>1575918151862304768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_3</td>\n",
       "      <td>1575918140839673873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_4</td>\n",
       "      <td>1575918135009738752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_5</td>\n",
       "      <td>1575918119251419136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc                   id\n",
       "0  doc_1  1575918182698979328\n",
       "1  doc_2  1575918151862304768\n",
       "2  doc_3  1575918140839673873\n",
       "3  doc_4  1575918135009738752\n",
       "4  doc_5  1575918119251419136"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# define the dataset location\n",
    "filename = 'tweet_document_ids_map.csv'\n",
    "sep=\"\\t\"\n",
    "# Set Pandas to show all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Read the data as a dataframe\n",
    "data = pd.read_csv(filename,sep,names=[\"doc\", \"id\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ec934a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet text removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line=  line.lower() ## Transform in lowercase\n",
    "    line=  line.split() ## Tokenize the text to get a list of terms\n",
    "    tweet_text=[]\n",
    "    for word in line:\n",
    "        #let's try to maintain the links in the correct format for the last part\n",
    "        if \"https\" not in word: #we maintain the # and @ because have relevance and we delete all the punctuation\n",
    "            word = re.sub(r'[^\\w\\s#@]','', word)\n",
    "            word = re.sub(r'_','',word)\n",
    "\n",
    "        if word:\n",
    "            tweet_text.append(word) \n",
    "            \n",
    "    line=[word for word in tweet_text if not word in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line=[stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc78b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def create_index(tweets,data):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets \n",
    "    data -- dataframe for mapping the doc and the tweet\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}  # dictionary to map docs ids to tweets ids\n",
    "    for tweet in tweets:  # Remember, lines contain a dictionary of a tweet\n",
    "        tw_id = tweet['id']\n",
    "        terms = build_terms(tweet['full_text']) # tweet_text\n",
    "        for i in range(len(data)):\n",
    "            if data['id'][i] == tw_id:\n",
    "                doc_id = data['doc'][i]\n",
    "        \n",
    "    \n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { ‚Äòterm1‚Äô: [current_doc, [list of positions]], ...,‚Äòterm_n‚Äô: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‚Äòweb‚Äô: [1, [0]], ‚Äòretrieval‚Äô: [1, [1,4]], ‚Äòinformation‚Äô: [1, [2]]}\n",
    "\n",
    "        ## the term ‚Äòweb‚Äô appears in document 1 in positions 0, \n",
    "        ## the term ‚Äòretrieval‚Äô appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_doc_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains tweet_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_doc_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                current_doc_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_doc_index[term]=[doc_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current doc index with the main index\n",
    "        for term_doc, posting_doc in current_doc_index.items():\n",
    "            index[term_doc].append(posting_doc)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1518f3",
   "metadata": {},
   "source": [
    "Here we create the index and we pass to the function de data of the csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e20db29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 46.67 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = create_index(lines,data)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70695c71",
   "metadata": {},
   "source": [
    "We print the index results for the term internet and we can see the doc_id and the corresponding position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1499c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index results for the term 'internet': [['doc_20', array('I', [7])], ['doc_46', array('I', [12])], ['doc_146', array('I', [6])], ['doc_380', array('I', [3])], ['doc_498', array('I', [9])], ['doc_558', array('I', [8])], ['doc_607', array('I', [4])], ['doc_1034', array('I', [7])], ['doc_1470', array('I', [18])], ['doc_1920', array('I', [9])], ['doc_2002', array('I', [3, 22])], ['doc_2756', array('I', [0])], ['doc_2797', array('I', [6])], ['doc_3081', array('I', [11])], ['doc_3223', array('I', [2])], ['doc_3269', array('I', [15])], ['doc_3563', array('I', [13])], ['doc_3797', array('I', [3])], ['doc_3854', array('I', [0])], ['doc_3867', array('I', [12])], ['doc_3929', array('I', [10])], ['doc_3981', array('I', [9])]]\n",
      "\n",
      "First 10 Index results for the term 'internet': \n",
      "[['doc_20', array('I', [7])], ['doc_46', array('I', [12])], ['doc_146', array('I', [6])], ['doc_380', array('I', [3])], ['doc_498', array('I', [9])], ['doc_558', array('I', [8])], ['doc_607', array('I', [4])], ['doc_1034', array('I', [7])], ['doc_1470', array('I', [18])], ['doc_1920', array('I', [9])]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Index results for the term 'internet': {}\\n\".format(index['internet']))\n",
    "print(\"First 10 Index results for the term 'internet': \\n{}\".format(index['internet'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb8f8a",
   "metadata": {},
   "source": [
    "We reuse the search function, which is temporal because later we will implement a better one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31b5af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(set(term_docs))\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e24d7999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "internet\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 22 for the searched query:\n",
      "\n",
      "document_id= doc_3981 \n",
      "document_id= doc_46 \n",
      "document_id= doc_20 \n",
      "document_id= doc_558 \n",
      "document_id= doc_2756 \n",
      "document_id= doc_498 \n",
      "document_id= doc_380 \n",
      "document_id= doc_1920 \n",
      "document_id= doc_3269 \n",
      "document_id= doc_2002 \n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"document_id= {} \".format(d_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab907dd5",
   "metadata": {},
   "source": [
    "We create the index tfidf (also provided with the class code ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e029d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    num_documents -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the input document\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tw_id = tweet['id']\n",
    "        terms = build_terms(tweet['full_text'])\n",
    "        for i in range(len(data)):\n",
    "            if data['id'][i] == tw_id:\n",
    "                doc_id = data['doc'][i]\n",
    "\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_doc_index ==> { ‚Äòterm1‚Äô: [current_doc, [list of positions]], ...,‚Äòterm_n‚Äô: [current_doc, [list of positions]]}\n",
    "\n",
    "\n",
    "        ## current_page_index ==> { ‚Äòweb‚Äô: [1, [0]], ‚Äòretrieval‚Äô: [1, [1,4]], ‚Äòinformation‚Äô: [1, [2]]}\n",
    "\n",
    "        ## the term ‚Äòweb‚Äô appears in document 1 in positions 0, \n",
    "        ## the term ‚Äòretrieval‚Äô appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_doc_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains tweet text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_doc_index[term][1].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_doc_index[term] = [doc_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        # normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_doc_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_doc_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1]) / norm, 4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        #merge the current doc index with the main index\n",
    "        for term_doc, posting_doc in current_doc_index.items():\n",
    "            index[term_doc].append(posting_doc)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents / df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab6fe7",
   "metadata": {},
   "source": [
    "We generate the new indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea7e3c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the TD-IDF index: 282.44 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_documents = len(lines)\n",
    "index, tf, df, idf = create_index_tfidf(lines, num_documents)\n",
    "print(\"Total time to create the TD-IDF index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b84f71",
   "metadata": {},
   "source": [
    "We rank the documents with the provided function in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a9a4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        # TODO: check how to vectorize the query\n",
    "        # query_vector[termIndex]=idf[term]  # original\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c5034",
   "metadata": {},
   "source": [
    "We get the ranked list of docs by getting the id of the docs that contain the query term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce8877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs = [posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3052e0ad",
   "metadata": {},
   "source": [
    "We create an auxiliary function to list the hashtags of a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "392c079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_hashtags(tweet):\n",
    "    hashtags = []\n",
    "    for tag in tweet[\"entities\"][\"hashtags\"]: #we loop the hashtags of the tweet that we can find in tweet[\"entities\"][\"hashtags\"]\n",
    "        hashtags.append(\"#\"+tag[\"text\"])\n",
    "    return ' '.join(hashtags).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8f633",
   "metadata": {},
   "source": [
    "We create a function to display the relevant information of the tweet ( we also added the document to see it works properly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2088a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_display(tweets, idd,data):\n",
    "    for i in range(len(data)):\n",
    "        if data['doc'][i] == idd: #we map the doc_id to the tweet_id\n",
    "            doc = data['doc'][i]\n",
    "            tw_id = data['id'][i]\n",
    "            for tweet in tweets: #we loop all tweets until we find the one with the corresponding tweet_id, then we display\n",
    "                if (tweet['id']) == tw_id:\n",
    "                    return\"Document : \"+ str(doc)+  \"|\" +\" Tweet: \" + str(tweet['full_text']) + \"|\" + \"Username: \" + str(tweet[\"user\"][\"name\"]) + \"|\" + \"Date: \"+ str(tweet[\"created_at\"]) + \"|\" + \"Hashtags: \" + list_hashtags(tweet) + \"|\" + \"Likes: \" +  str(tweet[\"favorite_count\"]) + \"|\" + \"Retweets: \"+ str(tweet[\"retweet_count\"]) + \"|\" + \"Url: \" + \"twitter.com/\"+str(tweet[\"user\"][\"id\"])+\"/status/\"+tweet['id_str']+\"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0182426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "internet\n",
      "\n",
      "======================\n",
      "Top 10 results out of 22 for the searched query:\n",
      "\n",
      "Document : doc_2756| Tweet: the internet is gold. #HurricaneIan #tryguys https://t.co/lj0eg8YxnE|Username: D√©sir√©e|Date: Fri Sep 30 15:36:25 +0000 2022|Hashtags: #HurricaneIan #tryguys|Likes: 12|Retweets: 0|Url: twitter.com/23288823/status/1575872198698418176\n",
      "\n",
      "Document : doc_2797| Tweet: #HurricaneIan My grandparents house is currently inhabitable. We will be out of power and internet for weeks. https://t.co/oGaIP8BfBN|Username: üçú|Date: Fri Sep 30 15:34:05 +0000 2022|Hashtags: #HurricaneIan|Likes: 1|Retweets: 1|Url: twitter.com/519197164/status/1575871613890924544\n",
      "\n",
      "Document : doc_2002| Tweet: I'm back! Sort of. Internet is still iffy, but they're obviously working on it. Lost power Wednesday evening. Returned home from my friend's place yesterday, power finally working this morning. Internet and cell service still spotty. Wow, what an experience! #HurricaneIan|Username: Theo Fenraven|Date: Fri Sep 30 16:26:29 +0000 2022|Hashtags: #HurricaneIan|Likes: 4|Retweets: 0|Url: twitter.com/66267328/status/1575884800627560449\n",
      "\n",
      "Document : doc_558| Tweet: Power officially out now! Oh no!!!\n",
      "#HurricaneIan (this is posted after the fact due to losing internet - ack!) https://t.co/4JvGhQgrXs|Username: Caroline Makes Music üíñ Bunny Girl VSinger! üíñ|Date: Fri Sep 30 18:11:21 +0000 2022|Hashtags: #HurricaneIan|Likes: 1|Retweets: 0|Url: twitter.com/216472343/status/1575911192090259457\n",
      "\n",
      "Document : doc_380| Tweet: @Xfinity #hurricaneian when will cable and internet be restored in #themeadows #Sarasota ? Frustrated that there has been no communication regarding the outages‚Ä¶|Username: Sun Coast Web Studio üá∫üá¶|Date: Fri Sep 30 18:20:41 +0000 2022|Hashtags: #hurricaneian #themeadows #Sarasota|Likes: 0|Retweets: 0|Url: twitter.com/526389667/status/1575913540636270592\n",
      "\n",
      "Document : doc_607| Tweet: Rain is pouring down, power is flickering, internet out, gusts of wind shaking our ‚Äútiny house.‚Äù #HurricaneIan https://t.co/F9OwhLhAzG|Username: David Kennard|Date: Fri Sep 30 18:09:14 +0000 2022|Hashtags: #HurricaneIan|Likes: 1|Retweets: 0|Url: twitter.com/23654711/status/1575910656234074112\n",
      "\n",
      "Document : doc_3854| Tweet: My internet on my phone is finally working again barely. Still no power everything is flooded but we are safe. This hurricane was a nightmare #HurricaneIan|Username: Randi Hilsercop|Date: Fri Sep 30 14:40:36 +0000 2022|Hashtags: #HurricaneIan|Likes: 1|Retweets: 0|Url: twitter.com/178163508/status/1575858151806889984\n",
      "\n",
      "Document : doc_3081| Tweet: @bennyjohnson Unfortunately, before the 21st century there were no cameras and smartphones everywhere to record weather phenomena and publish them on the Internet \n",
      "#ClimateChange #HurricaneIan|Username: Mauro|Date: Fri Sep 30 15:21:38 +0000 2022|Hashtags: #ClimateChange #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/2308461984/status/1575868478946672641\n",
      "\n",
      "Document : doc_46| Tweet: All is safe and well here at Valor Robotics. Our team made it through #HurricaneIan with minimal facility damage and loss of power and internet. (1/3) https://t.co/DJ3LPK0BRU|Username: Valor Robotics|Date: Fri Sep 30 18:36:04 +0000 2022|Hashtags: #HurricaneIan|Likes: 1|Retweets: 0|Url: twitter.com/1557458571754864642/status/1575917411001200640\n",
      "\n",
      "Document : doc_146| Tweet: A day and a half later after #HurricaneIan came and went our internet suddenly goes out.  no power, no running water, and now no internet/TV AT&amp;T what gives?|Username: Jennifer Juniper|Date: Fri Sep 30 18:31:29 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/2371240188/status/1575916255600689191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
